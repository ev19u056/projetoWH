
 **Model_Ver_3** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 250 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_4** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   

 
 **Model_Ver_5** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_6** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_8** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_9** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   ; fraction: 0.1; PCA implemented
 
 
 **Model_Ver_10** : Neuron-Layers: 53 54 60 40 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_11** : Neuron-Layers: 53 54 60 40 30 20 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_12** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 200 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_13** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 300 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_14** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 200 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.549666696331 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_15** : Neuron-Layers: 53 54 60 30 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.583472537327 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_16** : Neuron-Layers: 53 54 60 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.439469690196 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_17** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.585438985289 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_18** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 1.71947194709 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_19** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.4711463794 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_20** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 27.4933407095 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_21** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.966058075 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_22** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.003 ; Final_lr: ... ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.3779066976 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_23** : Neuron-Layers: 53 54 60 40 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: ... ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 1.93176214303 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_24** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: ... ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 5.60765530899 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_25** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: ... ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 16.0981033823 ; Weight Initializer: glorot_uniform DataSet: 0.3
 
 
 **Model_Ver_26** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: 9.99999901978e-05; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.77529172947 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_27** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: 9.99999901978e-05; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 20.3638748187 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_28** : Neuron-Layers: 53 71 71 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; BatchNormalization: False ; Batch size: 3000 ; Epochs: 33 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 6.50097623788 ; Weight Initializer: he_normal
LR_list: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05, 9.99999901978299e-05]
Dev_loss:  1.08199346552e-06   Dev_acc:  0.873116008629
Val_loss:  1.16608407519e-06   Val_acc:  0.869890281535
Test_loss: 1.16498942585e-06   Test_acc: 0.871672523977
Training_time: 722.56

 
 **Model_Ver_29** : Neuron-Layers: 53 71 71 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; BatchNormalization: False ; Batch size: 3000 ; Epochs: 32 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 6.54801102881 ; Weight Initializer: he_normal
LR_list: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295, 0.0009999999310821295]
Dev_loss:  1.11243098712e-06   Dev_acc:  0.872290981128
Val_loss:  1.16885833457e-06   Val_acc:  0.869575983847
Test_loss: 1.17150897865e-06   Test_acc: 0.870939169691
Training_time: 697.38
