
 **Model_Ver_3** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 250 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_4** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 

 
 **Model_Ver_5** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_6** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 250 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_6** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_8** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_9** : Neuron-Layers: 53 54 60 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Weight Initializer: glorot_uniform   ; fraction: 0.1; PCA implemented
 
 
 **Model_Ver_10** : Neuron-Layers: 53 54 60 40 30 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_11** : Neuron-Layers: 53 54 60 40 30 20 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_12** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 200 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_13** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 300 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.0 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_14** : Neuron-Layers: 53 54 60 30 15 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 200 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.549666696331 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_15** : Neuron-Layers: 53 54 60 30 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.583472537327 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_16** : Neuron-Layers: 53 54 60 15 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.439469690196 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_17** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.585438985289 ; Weight Initializer: glorot_uniform   
 

 
 **Model_Ver_18** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 1.71947194709 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_19** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.4711463794 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_20** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 27.4933407095 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_21** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.966058075 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_22** : Neuron-Layers: 53 54 60 30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.003 ; Final_lr: <tf.Variable 'Adam/lr:0' shape=() dtype=float32_ref>; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 30.3779066976 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_23** : Neuron-Layers: 53 54 60 40  30 20 10 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: <tf.Variable 'Adam/lr:0' shape=() dtype=float32_ref>; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 1.93176214303 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_24** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: <tf.Variable 'Adam/lr:0' shape=() dtype=float32_ref>; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 5.60765530899 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_25** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 120 ; Initial_lr: 0.01 ; Final_lr: <tf.Variable 'Adam/lr:0' shape=() dtype=float32_ref>; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 16.0981033823 ; Weight Initializer: glorot_uniform DataSet: 0.3   
   

 
 **Model_Ver_26** : Neuron-Layers: 53 71 71 71 1 ; Activation: relu ; Output: Sigmoid ; Batch size: 3000 ; Epochs: 5 ; Initial_lr: 0.01 ; Final_lr: 0.00999999977648; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 8.79388202449 ; Weight Initializer: he_normal   
 